\chapter{Random Variables and Experiments} \label{ch:pbbasics}

This chapter introduces the basic concepts, axioms and theorems in probability theory.

\section{Randomness and Stochasticity}

People use \textit{random} and \textit{stochastic} to describe a variable or a model whose measurements or outcomes are not precisely predictable.

By saying \textit{randomness}, we often refer to the case where a variable (as in ``random variable'') is not predictable. Its value is not known precisely until it is measured. The chance of it taking particular values may follow some patterns which can be described as the statistic property of the variable. For example, the result of tossing a coin is a random variable with the following statistic property. It is either head ($X=1$) or tail ($X=0$), each with a $50\%$ chance. The precise value remains unknown until the coin is tossed.

By saying \textit{stochasticity}, we often refer to the case where the outcome of a process (as in ``stochastic process'') is not predictable. This is likely caused by the incomplete modeling or random disturbance of the system. As a result, the process becomes non-deterministic, i.e., the output of the system cannot be precisely determined by the model and the input. A stochastic process can still be described by a parametric model, but with some unknowns (usually in the form of random variables) in the equations.

This part of the notebook mainly studies random variables. Stochastic process and control of stochastic systems are introduced elsewhere in control system related notebooks.

\subsection{Random Experiments}

``Experiment'' is one of the most important activities in science and engineering. Very often, experiments are used to verify a theory. In these cases, experiments are carefully designed so that its outcome is deterministic and predictable as long as the theory is correct. By observing the results of the experiments matching the prediction of the theory, we build confidence in the theory.

However, there are other experiments where we do not have full control over their results due to the lack of information or incomplete modeling. Such experiments include tossing a coin, predicting the GDP of a country next year, etc. These experiments are known as \textit{random experiments}. The result of a random experiments can still be meaningful because it reflects some insights of the system. The challenge is to design the experiments so that useful information can be revealed from the result efficiently.

\subsection{Sample Space}

A set $S$ that consists of all possible outcomes of a random experiment is called a \textit{sample space}.

If a sample space has a finite number of elements, it is called a \textit{finite sample space}. Otherwise, it is called an \textit{infinite sample space}. If the elements in an infinite sample space can be mapped to natural numbers, the sample space is also called a \textit{countably infinite sample space}. Otherwise, the sample space is called a \textit{noncountably infinite sample space}. Examples are given below.
\begin{itemize}
	\item Finite sample space. Randomly pick 5 balls out of a bag that contains 50 red balls and 50 green balls. The number of red balls in the picked 5 balls forms a finite sample space.
	\item Countably infinite sample space. The number of continuous ``heads'' when tossing a coin before the first ``tail'' forms a countably infinite sample space.
	\item Noncountably infinite sample space. The precise time consumption it takes for a PC to start forms a noncountably infinite sample space.
\end{itemize}

\begin{shortbox}
\Boxhead{Countable Infinity VS Uncontable Infinity}
Though both infinity, the total number of natural numbers, i.e., the cardinality of $\mathbb{N}$, is less than the total number of real numbers, i.e., the cardinality of $\mathbb{R}$.

The cardinality of $\mathbb{N}$ is known as $\aleph_0$. It is also the cardinality of all rational numbers. In computing, it is also the cardinality of all computable numbers (i.e., numbers that can be computed to any desired precision by a finite terminating algorithm) and computable functions (i.e., algorithms). The total number of real numbers is known as $\aleph_1$. It is also the cardinality of all irrational numbers and complex numbers, the number of points on a line or in a high dimensional space $\mathbb{R}^n$ where $n$ is a finite integer.

Larger infinity quantities such as $\aleph_2, \aleph_3, \ldots$ are also defined, though they may not have an intuitive explanation as $\aleph_0$ and $\aleph_1$.

Deeper discussion of this topic requires advanced set theory and is not given in this notebook.
\end{shortbox}

Finite sample space and countably infinite sample space are also known as \textit{discrete sample space}, whereas noncountably infinite sample space, \textit{nondiscrete sample space}.

\subsection{Events}

An event is a subset $A \subseteq S$, i.e., it is a portion of all possible outcomes. An event may or may not occur depending on the outcome of the experiment. In the special case where $A$ has only one element, the event is also called an \textit{elementary event}, or a \textit{sample}. Notice that all elementary events are mutually exclusive as each of them contains an independent outcome of the experiment hence cannot occur all together. In the special case where $A=S$, the event is also called a certain event.

\section{Probability}

Given an experiment and an event, it is unsure whether the event will occur, and the \textit{probability} is a measurement of the likelihood that the event is going to occur. For example, if the probability is $50\%$, it means that the event has an equal chance of happening or not happening.

\subsection{Classical and Empirical Probability}

There are different ways to calculate or estimate the probability of an event. 

In the \textit{classical approach}, it is assumed that all elementary events have the same chance of occurrence, and the total number of the elementary events is a finite and known number, $n$. Define an event. If the event contains $h$ elementary events, then the \textit{classical probability} of the event is given by $h/p$.

Calculating the cardinality of event $A$ and sample space $S$ is the key to solving the classical probability. \textit{Permutation} and \textit{combination} are widely used for such calculations. Suppose that there are $n$ distinct objects, and we would like to select $r$ objects from them and put them into a sequence. The permutation
\begin{eqnarray}
  nPr &=& n(n-1)(n-2)\cdots (n-r+1) \label{eq:permutation}
\end{eqnarray}
gives the number of possible outcomes. In the special case $r=n$,
\begin{eqnarray}
  nPn &=& n(n-1)(n-2)\cdots \times 1 \nonumber
\end{eqnarray}
where $n(n-1)(n-2)\cdots \times 1$ is often denoted as $n!$. Therefore, \eqref{eq:permutation} becomes
\begin{eqnarray}
  nPr &=& \dfrac{n!}{(n-r)!} \nonumber
\end{eqnarray}
In permutation, the arrangement of the selected $r$ objects matters. When the order does not matter, combination
\begin{eqnarray}
  nCr &=& \dfrac{nPr}{r!} \nonumber \\
  &=& \dfrac{n!}{r!(n-r)!} \nonumber
\end{eqnarray}
is used to calculate the total number of outcomes. Notice that $nCr$ is also denoted by $\left(\begin{array}{c}
                                                                                           n \\
                                                                                           r
                                                                                         \end{array}\right)$.

In the \textit{frequency approach}, we can repeat the experiment $n$ times where $n$ is a large number, and record the number of instants where the event happened as $h$. The \textit{empirical probability} of the event is obtained by calculating $h/p$ which should converge to the true probability as $n$ goes large.

\subsection{Geometric Probability}

\text{Geometric probability} is an extension of the classical probability to continuous random variables. Similar with classical probability where it is prerequisite that all elementary events share the same probability, in geometric probability it is assumed that all random variables (usually 2 or 3 variables in total) are uniformly distributed. The probability of an event happening then can be obtained by measuring the area or volume associated with the event.

An example is used to illustrate the use of geometric probability.

\begin{shortbox}
\Boxhead{Geometric Probability Example}
Consider two people trying to meet up at the park, but they forgot to tell each other the time to meet. Both of them will arrive at the park at anytime between $8:00$ AM and $9:00$ AM with equal chance, and wait for the other for 15 minutes or until $9:00$ AM, whichever is earlier, and then will leave the park if the other does not show up.

Calculate the chance of the two people successfully meeting up.

Draw the sample space in a 2-D plot as shown in Fig. \ref{fig:geometricprobexp}, where the x-axis and y-axis are the arriving time of the two people, respectively. The shaded area represents the samples where the two would meet up successfully.

Divide the shared area by the total sample space area to get $P=7/16=43.75\%$ which is the probability that the two would meet up successfully.
\end{shortbox}

\begin{figure}
	\centering
	\includegraphics[width=250pt]{chapters/ch-random-variables/figures/geometricprobexp.png}
	\caption{Sample space of two people arriving at part from 8:00 AM to 9:00 AM.} \label{fig:geometricprobexp}
\end{figure}

\subsection{Axioms}

The \textit{axiomatic approach} puts things into mathematical prospective as follows. It is shown later that the classical probability can be illustrated clearly using axiomatic approach.

Let the sample space be denoted by $S$, and events by $A_i$. For simplicity of illustration, assume that $S$ is discrete. Define $P(\cdot)$ as the \textit{probability function} and $P(A_i)$ the probability of the event $A_i$, subject to the following axioms:
\begin{enumerate}
  \item For every event $A_i$, $P(A_i)\geq 0$.
  \item For the certain event $S$, $P(S)=1$.
  \item For mutually exclusive events $A_1$ and $A_2$, $P\left(A_1\cup A_2\right) = P(A_1)+P(A_2)$.
\end{enumerate}

Using the above axioms, a bunch of well-known theorems can be derived, such as
\begin{itemize}
  \item If $A_1 \subseteq A_2$, $P(A_2-A_1) = P(A_2)-P(A_1)$.
  \item For every event $A_i$, $0\leq P(A_i) \leq 1$.
  \item For the impossible event $\varnothing$, $P(\varnothing)=0$.
  \item For the complement of an event $A\textprime$, $P(A\textprime)=1-P(A)$.
  \item For mutually exclusive events $A_i, i=1,...,n$, if $A = \bigcup_{i=1}^{n} A_i$, $P(A) = \sum_{i=1}^{n}P(A_i)$.
  \item For two events $A_1$ and $A_2$, $P\left(A_1\cup A_2\right) = P(A_1)+P(A_2)-P\left(A_1\cap A_2\right)$.
\end{itemize}

With the above axioms, we can revisit classical probability as follows. Assume that a discrete and finite sample space $S$ consists of the following elementary events (elementary events are always mutually exclusive) $A_i, i=1,...,n$, i.e.,
\begin{eqnarray}
  S &=& \bigcup_{i=1}^{n} A_i \nonumber
\end{eqnarray}
and assume equal probabilities for all the elementary events, i.e., the probability of each event is given by
\begin{eqnarray}
  P(A_i) &=& \dfrac{1}{n}, i=1,...,n \nonumber
\end{eqnarray}
Define an event $A$ that is made up of $h$ such elementary events out of $A_i$. The probability of $A$ can then be calculated by
\begin{eqnarray}
  P(A) &=& \dfrac{h}{n} \nonumber
\end{eqnarray}
where $h$, $n$ are the cardinality of $A$ and $S$ with respect to the elementary events.

\subsection{Conditional Probability}

Assume two events $A$ and $B$. The \textit{conditional probability} $P(B|A)$ describes the probability of $B$ given that $A$ has occurred. The definition is given below. Think of this definition as $S$ replaced by $A$ since $A$ has been confirmed occurred.
\begin{eqnarray}
  P(B|A) &\equiv& \frac{P(A\cap B)}{P(A)} \nonumber
\end{eqnarray}
Or equivalently,
\begin{eqnarray}
  P(A\cap B) &=& P(A)P(B|A) \nonumber
\end{eqnarray}
From the above,
\begin{eqnarray}
  P(B|A) &=& \dfrac{P(B)P(A|B)}{P(A)} \nonumber
\end{eqnarray}
which is known as the Bayes' rule.

In the special case where $P(B|A)=P(B)$, or equivalently $P(A\cap B) = P(A)P(B)$, the two events $A$ and $B$ are said to be \textit{independent events}.
