\chapter{Random Variables and Distributions} \label{ch:rv}

The definition of random variable has been introduced in the previous chapter. This chapter digs deeper into the different types of random variables, their properties, and how the they are described mathematically.

\section{Discrete and Continuous Random Variables}

A random variable may be discrete or continuous depending on the sample space of the variable.

\subsection{Discrete Random Variables}

If a random variable $X$ takes only discrete values $x_1, x_2, ...$, it is called a \textit{discrete random variable}.
The probability of $X$ taking a particular value $x$ is denoted by $P(X=x)$ or simply $f(x)=P(X)$ if no ambiguity. In this case, $P(X=x)$ and $f(x)$ are called the \textit{probability function} (also known as \textit{probability mass function}) of $X$.

Furthermore, define $P(X\leq x)$ or $F(x)$ as the \textit{cumulative distribution function} of $x$. It is easy to prove that $F(x)$ is nondecreasing, and $\lim_{x\rightarrow -\infty}F(x)=0$, $\lim_{x\rightarrow \infty}F(x)=1$. Also, $F(x)$ ``jumps'' at each $P(X=x_k)>0$ and it is continuous from the right.

\subsection{Continuous Random Variables}

A random variable $X$ may also take continuous values in many applications. For example, let $X$ denote the time consumption to finish a task. In this case, $x$ is a random variable whose value can be any positive number.

In this case, the chance for $X$ to take a precise value, say for the student to finish his homework using precisely 25 minutes 13 seconds and 750 milliseconds, is very small (in fact, zero). The probability function $P(X=x)$ becomes pointless. The cumulative distribution function $F(x) = P(X\leq x)$ still makes sense, as it calculates the probability of $X$ given by not a precise value but within a range.

Inspired by this, define \textit{probability density function (PDF)} or $f(x)$ for continuous random variable as follows. The probability density function $f(x)$ is such that
\begin{eqnarray}
  P(a < X < b) &=& \int_{a}^{b}f(x)dx \nonumber
\end{eqnarray}
Therefore,
\begin{eqnarray}
  F(x) &=& P(X\leq x) \nonumber \\
  &=& \int_{-\infty}^{x} f(\epsilon)d\epsilon \nonumber
\end{eqnarray}
Notice that $f(x)$ itself is not probability. It is $f(x)dx$ accumulating in range $x \in (a, b)$ that forms the probability, hence the name ``probability density''.

In science and engineering problems, continuous random variables are more common than discrete random variables. In engineering, discrete random variables can sometimes be described by impulse PDF.

\section{Joint Distributions}

The joint distribution refers to the distribution of multiple random variables. It is especially useful when these variables are correlated, in which case the joint probability function or PDF can reflect their correlation. For example, in a system identification problem the unknown system parameters are correlated with the measurements and their correlations are described by their joint PDF, and the filter uses the joint PDF and the measurements to estimate the unknown parameters.

\subsection{Joint Probability}

Consider two random variables $X$ and $Y$. In the case of discrete variables, define joint probability function and joint cumulative distribution function as follows.
\begin{eqnarray}
  f(x, y) &=& P\left(X=x, Y=y\right) \nonumber \\
  F(x, y) &=& P\left(X\leq x, Y\leq y\right) \nonumber \\
  &=& \sum_{u\leq x}\sum_{v\leq y}f(u, v) \nonumber
\end{eqnarray}

In the case of continuous random variables, joint PDF of $X$ and $Y$ is defined such that
\begin{eqnarray}
  \int_{x=a}^{b}\int_{y=c}^{d} f(x, y) dxdy &=& P\left(a < X < b, c < y < d \right) \label{eq:jointpdf}
\end{eqnarray}
Integrate \eqref{eq:jointpdf} w.r.t. one variable from $-\infty$ to $\infty$ gives the PDF of the other variable. For example,
\begin{eqnarray}
  f_X(x) &=& \int_{y=-\infty}^{\infty} f(x, y) \label{eq:jointpdfdowngrade}
\end{eqnarray}
gives the PDF of $X$ alone.

We can define cumulative distribution function for joint distribution likewise as follows.
\begin{eqnarray}
  F(x, y) &=& P(X \leq x, Y \leq y) \nonumber \\
  &=& \int_{u=-\infty}^{x}\int_{v=-\infty}^{y}f(u, v)dudv \nonumber \\
  F_X(x) &=& P(X \leq x) \nonumber \\
  &=& \int_{u=-\infty}^{x}\int_{v=-\infty}^{\infty}f(u, v)dudv \nonumber
\end{eqnarray}

\subsection{Conditional Distributions}

Equation \eqref{eq:jointpdfdowngrade} gives the distribution of $X$ when there is no measurement of $Y$. Conditional distribution, on the other hand, tries to find the distribution of $X$ when $Y$ is measured. For example, in \eqref{eq:jointpdf}, calculate $f_{X|Y}(x |Y=y)$, i.e., the PDF of $X$ given $Y=y$. In many literatures, $f_{X|Y}(x |Y=y)$ is denoted by $f_{X|Y}(x|y)$ for simplicity.

The conditional PDF $f_{X|Y}(x|y)$ can be obtained as follows. It is essentially Bayes' rule applied on continuous variables.
\begin{eqnarray}
  f_{X|Y}(x|y) &=& \dfrac{f(x, y)}{f_Y(y)} \label{eq:conditionalpdf1}
\end{eqnarray}
where $f_Y(y)$ is obtained using \eqref{eq:jointpdfdowngrade}. Equation \eqref{eq:conditionalpdf1} is a function of both $x$ and $y$. Substitute the measured value of $y$ into \eqref{eq:conditionalpdf1} and it reduces to the PDF of $x$ alone. Its integration w.r.t. $x$ from $-\infty$ to $\infty$, therefore, is $1$. This can be easily verified as follows.
\begin{eqnarray}
  \int_{x=-\infty}^{\infty}f_{X|Y}(x|y)dx &=& \int_{x=-\infty}^{\infty}\dfrac{f(x, y)}{f_Y(y)}dx \nonumber \\
  &=& \dfrac{\int_{x=-\infty}^{\infty}f(x, y)dx}{f_Y(y)} \nonumber \\
  &=& \dfrac{f_Y(y)}{f_Y(y)} \nonumber \\
  &=& 1 \nonumber
\end{eqnarray}
Notice that given a particular value of $y$, $f_Y(y)$ is a constant value independent of $x$, hence can be taken out from the integration in the above derivation.

If $X$ and $Y$ are independent variables, $f(x,y) = f_X(x)f_Y(y)$. In this case, \eqref{eq:conditionalpdf1} becomes
\begin{eqnarray}
  f_{X|Y}(x|y) &=& f_X(x) \nonumber
\end{eqnarray}
which implies that the information of $Y=y$ does not affect our understanding of $X$, just as if the information is absent.

\subsection{Parameter Estimation with Conditional Distribution}

Equation \eqref{eq:conditionalpdf1} is widely used in parameter estimation. Let $\theta$ be the parameter to be estimated, and $x$ the measurement that reflects $\theta$ via measurement model $f(\theta, x)$ which is given in the form of joint PDF. Both $\theta$ and $x$ can be vectors.

Without measurement $x$, the estimate of $\theta$ is given by
\begin{eqnarray}
	\hat{\theta} &=& \int_{-\infty}^{\infty} f_\theta(\theta) d\theta \nonumber
\end{eqnarray}
where $f_\theta(\theta)$ is obtained using \eqref{eq:jointpdfdowngrade}. This is known as the \textit{priori estimation} of $\theta$. 

Given measurement $x$, the \textit{posteriori estimation} of $\theta$ can be obtained as follows. From \eqref{eq:conditionalpdf1},
\begin{eqnarray}
  f_{\theta|X}(\theta|x) &=& \dfrac{f_{X|\theta}(x|\theta)f_\theta(\theta)}{f_X(x)} \label{eq:conditionalpdf2} \\
  \hat{\theta} &=& \int_{-\infty}^{\infty} f_{\theta|X}(\theta|x) d\theta \nonumber
\end{eqnarray}
where $f_{X|\theta}(x|\theta)$ is known as the \textit{likelihood function} that describes the likelihood of measuring $x$ if the actual parameter(s) is $\theta$. The PDF $f_\theta(\theta)$ is from the priori estimation of $\theta$. Finally, $f_X(x)$ is known as the \textit{evidence}. With fixed $x$, $f_X(x)$ becomes a constant.

Equation \eqref{eq:conditionalpdf2} is essentially
\begin{eqnarray}
  \textup{posteriori} &=& \dfrac{\textup{likelihood}\times\textup{priori}}{\textup{evidence}} \label{eq:posteriorimemo}
\end{eqnarray}

Terms \textit{priori} and \textit{prior} can be used interchangeably, and so are \textit{posteriori} and \textit{posterior}.

\section{Probability of Derived Variables}

Let $X$ be a random variable with PDF $f_X(x)$. Let $U$ be another random variable which is a function of $X$ via $U=\phi(X)$. The PDF of $U$ can be calculated as follows. For simplicity, assume that $U=\phi(X)$ is a injective function (one-to-one function), and $X=\phi^{-1}(U)=\psi(U)$. In that case, the PDF of $U$, $g(u)$, can be obtained as follows.
\begin{eqnarray}
  g(u) &=& \left|\psi\textprime(u)\right|f\left(\psi(u)\right) \nonumber
\end{eqnarray}
For example, let $U=aX$, $X=\frac{U}{a}$.
\begin{eqnarray}
  g(u) &=& \dfrac{1}{a}f\left(\dfrac{u}{a}\right) \nonumber
\end{eqnarray}

Let $X$, $Y$ be two random variables with joint distribution $f(x, y)$. Let $U=X+Y$. The PDF of $U$ can be obtained as follows.
\begin{eqnarray}
  g(u) &=& \int_{-\infty}^{\infty} f(x, u-x)dx \label{eq:conditionalpdf3}
\end{eqnarray}
In the special case where $X$ and $Y$ are independent, $f(x, y) = f_X(x)f_Y(y)$, and \eqref{eq:conditionalpdf3} becomes
\begin{eqnarray}
  g(u) &=& \int_{-\infty}^{\infty} f_X(x)f_Y(u-x)dx \nonumber \\
  &=& f_X * f_Y \nonumber
\end{eqnarray}
where $*$ denotes the convolution operator.
