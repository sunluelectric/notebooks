\chapter{Python (Part II: Advanced)} \label{ch:tftorch}

This chapter focuses on the introduction of commonly used Python-supported ANN engines used in data science. ANN relationships with AI, machine learning and deep learning as well as theories and mechanisms behind ANN can be found on other notebooks, hence is not given here. The introduction only contains the basic usage of these ANN engines from the implementation perspective, and it may not reflect the state-of-the-art technologies such as transformer, LLM, instruction-tuned LLM, etc. These state-of-the-art technologies are introduced on other notebooks.

There are many ANN engines for Python. Among all, TensorFlow and PyTorch are very popular and powerful generic-purpose ANN engines. They both cover a large range of supervised, reinforcement and unsupervised learning applications including classification, regression, pattern recognition, computer vision, natural language processing, clustering, abnormality detection, and many more. They both offer variety of tools to quickly and flexibly design and deploy different types of AI models such as conventional dense networks, CNN models, RNN models, and many more. Both of them can be used to train, evaluate and run networks. Both of them provide server solutions, cloud solutions and edge computing solutions.  TensorFlow and PyTorch are introduced in this chapter.

Installation of TensorFlow and PyTorch can be found in there websites. Although it is possible to run all the calculations on CPU, these ANN engines are more powerful when GPU/TPU are enabled. Depends on the OS and the GPU/TPU brands of the local system, different methods may apply to enable GPU/TPU. For example, if NVIDIA GPU is used, a software called CUDA can be used to configure and enable GPU for ANN training. The installation of TensorFlow, PyTorch, and the enabling of GPU/TPU modules are not covered here.

Alternative to running the code on a local system, consider using online platforms such as Google Colaboratory, which already have all necessary packages pre-installed and the CPU/GPU/TPU pre-configured.

\section{Quick Review}

This section briefly reviews the basic concepts used in this chapter. Details of the concepts can be found elsewhere in other notebooks.

\subsection{AI Pipeline}

AI pipeline is a set of (automated) steps used to build, train, evaluate and deploy AI models. An AI pipeline usually includes at least the following steps (for supervised learning):
\begin{enumerate}
  \item Data collection
  \item Data preparation
  \item Model design
  \item Model training
  \item Model evaluation and analysis
  \item Model deployment and testing
\end{enumerate}
where notice that model design and training might need to be carried out iteratively. After the training, the performance of the model is validated using the validation set, according to which the model and its hyper parameters can be modified.

\subsection{Data Preparation and Model Evaluation}

Model training is where the magic happens. Nowadays, with the help of AI engines, it is done automatically via back propagation and other techniques. Given the same training data and model design (including training methods), the almost-the-same trained model can be reproduced by the machine. It is done in a standardized, systematic and consistent manner, hence does not distinguish the performance of the model.

It is rather the data preparation (pre-processing), model design, and model evaluation that require human guidance. Depending on the experience and skill level of the data scientist and engineer, this is where the model performance may differ. Model design and fine-tuning are closely related to model evaluation, as model evaluation results and analysis decides how the model should be tuned. Therefore, it can be concluded that good data preparation, model evaluation and analysis skills are the critical factors that affect model behavior.

To be more precise by breaking down the aforementioned critical factors:
\begin{itemize}
  \item Data preprocessing:
  \begin{itemize}
      \item Data cleaning. This is a critical step that greatly influences the model's performance. It includes cleaning the data, handling missing values, and dealing with outliers. Often, domain knowledge plays a significant role in these steps. In practice, some of the above procedures are done using AI engine built-in functions, while others are done using other packages such as pandas.
      \item Feature engineering. This involves creating new features from the existing data that might help improve the model's performance. Feature selection is also crucial to reduce overfitting and improve the model's interpretability. Again, domain knowledge can be very beneficial here.
  \end{itemize}
  \item Model design, evaluation and tuning:
  \begin{itemize}
      \item Model selection. Choosing the right model or architecture for the problem at hand requires a solid understanding of the strengths and weaknesses of different models.
      \item Hyperparameter tuning. While there are systematic approaches like grid search or random search, often, practical experience and intuition play a significant role in choosing the right hyperparameters.
      \item Model evaluation and analysis. Evaluating a model goes beyond looking at a single metric. It involves understanding the model's errors, checking its performance on different subsets of data, and considering aspects like fairness and interpretability.
  \end{itemize}
\end{itemize}

Given that many, if not all, of the above steps involve a lot of human interaction, data visualization tools often play a very important role to assist humans on their tasks.

\subsection{Commonly Seen ANN Use Cases}

\subsection{Computer Vision}

CV, as an important part of AI, has evolved in the past decades. In the early 2010s, ANN was not used in CV. Instead, conventional deterministic approaches were widely used. With the development in deep learning, the primary approach for CV has changed to CNN. There are a few milestones along the way that together make the change happen:
\begin{itemize}
  \item Development of GPU
  \item CNN with deep neural network
  \item Introduction of rectified linear unit (ReLU) activation function
  \item Regularization techniques
\end{itemize}
Recently, with the development in transformer model and large language model, CV is able to be combined with LLM for image comprehension, reasoning, and even artwork generation.

The commonly seen objectives of CV include:
\begin{itemize}
  \item Image classification
  \item Object detection
  \item Image generation
  \item Image search
  \item Image comprehension and generation
\end{itemize}

\subsection{Natural Language Processing}

\section{TensorFlow}

TensorFlow is an open-source software library for machine learning developed by Google in 2015. TensorFlow 2.X is released in 2019 and it is know the official latest major updated version. TensorFlow is backed up by a large community and it supports Python as well as a few other programming languages.

Don't confuse TensorFlow with Keras, later of which is a Python library built on top of deep learning libraries such as TensorFlow, and provides a simple and useful API. In TensorFlow 2.X, Keras is officially adopted as its API. Therefore, when TensorFlow 2.X is used, there is no need to install or import Keras separately.

\subsection{TensorFlow Basics}

Unless otherwise mentioned, the following packages are imported and the command executed in the beginning of all the relevant scripts.
\begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
import tensorflow as tf

tf.test.is_gpu_available()
\end{lstlisting}
where \verb|.is_gpu_available()| tests whether GPU is enabled on the machine.

The well-known \verb|numpy| package defines ``NumPy arrays'' to store vectors, matrices and tensors. Package \verb|tensorflow| also defines counterparts ``TensorFlow tensors'' for the same purpose. NumPy arrays and TensorFlow tensors can be converted from one to the other. A key difference of the two is that TensorFlow tensors related calculations are executed on GPU wherever possible, making it more efficient when comes to large-scale calculation that can be paralleled. An example is given below.
\begin{lstlisting}[language=Python]
x = np.array([1, 2, 3, 4, 5])
y = tf.convert_to_tensor(x, dtype=tf.float64)
x = x*0.3 // cpu calculation
y = y*0.3 // gpu parallel calculation
\end{lstlisting}

\subsection{Classification and Regression}

\vspace{0.1in}
\noindent \textbf{Classification}
\vspace{0.1in}



\vspace{0.1in}
\noindent \textbf{Regression}
\vspace{0.1in}

The following data frame \textit{kaggle.com/datasets/shree1992/housedata} is used in this example to as a demonstration to predict house pricing using regression model.

The following class \verb|SimpleRegressionModel| serves as an example that can be used for the above task. Notice that this model design is only a demonstration, and it is not optimized.
\begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import seaborn as sns

class SimpleRegressionModel:
	"""
	SimpleRegressionModel trains and tests a regression model using the given data frame.
	"""
	def __init__(self):
		self.num_input = None
		self.num_output = None
		self.scalar = None
		self.num_training = None
		self.X_train = None
		self.Y_train = None
		self.num_validation = None
		self.X_val = None
		self.Y_val = None
		self.history = None
		self.num_test = None
		self.X_test = None
		self.Y_test = None
		self.model = None
	def import_dataset(self, df_total: pd.DataFrame, iplist: list, oplist: list, validation_size: float, test_size: float):
        total_X = df_total[df_total.columns.intersection(iplist)]
        object_columns = total_X.select_dtypes(include=['object'])
        if not object_columns.empty:
            dummy_columns = pd.get_dummies(object_columns)
            total_X = pd.concat([total_X.drop(object_columns, axis=1), dummy_columns], axis=1)
        self.num_input = len(total_X.columns)
        total_Y = df_total[df_total.columns.intersection(oplist)]
        self.num_output = len(total_Y.columns)
        train_val_X, self.X_test, train_val_Y, self.Y_test = train_test_split(total_X, total_Y, test_size=test_size, random_state=None)
        self.num_test = len(self.X_test.index)
		if validation_size == 0:
			self.X_train = train_val_X
			self.Y_train = train_val_Y
			self.X_val = []
			self.Y_val = []
			self.num_training = len(self.X_train.index)
			self.num_validation = 0
		else:
			self.X_train, self.X_val, self.Y_train, self.Y_val = train_test_split(train_val_X, train_val_Y, test_size=validation_size/(1-test_size), random_state=None)
			self.num_training = len(self.X_train.index)
			self.num_validation = len(self.X_val.index)
		print("Dataset size: training: {}, validation: {}, test: {}".format(self.num_training, self.num_validation, self.num_test))
		self.scalar = MinMaxScaler()
		self.X_train = self.scalar.fit_transform(self.X_train)
		if validation_size == 0:
			pass
		else:
			self.X_val = self.scalar.transform(self.X_val)
		self.X_test = self.scalar.transform(self.X_test)
	def design_model(self, hidden_layer_model: list, optimizer: str, learning_rate: float, loss: str):
		"""
		The input model describes the design of the model. It is a list of layers. Each layer is given by a dictionary describing layer type, number of nodes, etc.
		"""
		self.model = tf.keras.Sequential()
		for ind in range(len(hidden_layer_model)):
			if ind == 0:
				if hidden_layer_model[ind]["type"] == "dense":
					layer = tf.keras.layers.Dense(hidden_layer_model[ind]["node"], activation = hidden_layer_model[ind]["activation"], input_shape = (self.num_input, ))
				else:
					pass
			else:
				if hidden_layer_model[ind]["type"] == "dense":
					layer = tf.keras.layers.Dense(hidden_layer_model[ind]["node"], activation = hidden_layer_model[ind]["activation"])
				else:
					pass
			self.model.add(layer)
		self.model.add(
			tf.keras.layers.Dense(self.num_output, activation='relu')
		)
		if optimizer == 'adam':
			optimizer = tf.keras.optimizers.Adam()
		self.model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])
	def train_model(self, epochs: int, batch_size: int):
		self.history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2)
	def evaluate_model(self):
		if self.num_validation == 0:
			print("Validation set is empty.")
		else:
			loss, mae = self.model.evaluate(self.X_val, self.Y_val, verbose=2)
			print("Validation set test result: loss={}, mae={}".format(loss, mae))
	def test_model(self):
		loss, mae = self.model.evaluate(self.X_test, self.Y_test, verbose=2)
		print("Test set test result: loss={}, mae={}".format(loss, mae))
\end{lstlisting}

The following code uses the above defined class to predict house price.
\begin{lstlisting}[language=Python]
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import seaborn as sns

df_house_pricing = pd.read_csv("house_pricing.csv").dropna()
srm = SimpleRegressionModel()
srm.import_dataset(
    df_total=df_house_pricing,
    iplist=["bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "yr_built", "yr_renovated", "condition", "city"],
    oplist=["price"],
    validation_size=0, test_size=0.2)
hidden_layer_model = [
	{
		"type": "dense",
		"node": 128,
		"activation": "relu"
	},
	{
		"type": "dense",
		"node": 64,
		"activation": "relu"
	},
	{
		"type": "dense",
		"node": 64,
		"activation": "relu"
	},
	{
		"type": "dense",
		"node": 32,
		"activation": "relu"
	},
	{
		"type": "dense",
		"node": 16,
		"activation": "relu"
	},
]
srm.design_model(hidden_layer_model=hidden_layer_model, optimizer='adam', learning_rate=0.001, loss='mse')
srm.train_model(epochs=100, batch_size=50)
srm.evaluate_model()
srm.test_model()
\end{lstlisting}

The above example is self-explanatory. Some key components of the codes are:
\begin{itemize}
  \item Use \verb|train_test_split()| from \verb|sklearn.model_selection| to split the data set into training set, validation set and testing set.
  \item Use \verb|MinMaxScaler()| from \verb|sklearn.preprocessing| to normalize the inputs to the AI model.
  \item Use \verb|tf.keras.Sequential()| and \verb|tf.keras.layers| to design the AI model structure.
  \item Use \verb|.compile()| to configure optimization engine, loss function, validation matrix, etc., during the training.
  \item Use \verb|.fit()| to train the model using the training set.
  \item Use \verb|.evaluate()| to evaluate the AI model performance.
  \item Use \verb|.predict()| to carry out prediction of input points.
\end{itemize}

Use \verb|.save("name")| to save a model, and \verb|load_model()| from \verb|tensorflow.keras.models| to load a model. Some popular formats to store a model include H5 file.




\subsection{Computer Vision}

\subsection{General Sequential Data Processing}

\subsection{Natural Language Processing}

\subsection{TensorFlow on Different Platforms}

\section{PyTorch}

TensorFlow is another open-source software library for machine learning originally developed by Meta AI in 2016. It is now under the Linux foundation umbrella.

\subsection{PyTorch Basics}

\subsection{Classification and Regression}

\subsection{Computer Vision}

\subsection{General Sequential Data Processing}

\subsection{Natural Language Processing}

\subsection{PyTorch on Different Platforms}

