\chapter{Model Predictive Control System} \label{ch:mpcs}

Model predictive control system (MPC) belongs to the broader family of optimal control system. It defines a cost function and a bunch of constraints about the states and control signals. The objective is to find such control signal trajectory that fulfills the constraints while minimizing the cost. The optimization problem is solved recurrently at each timestamp where only the first control input from the optimal sequence is actually applied to the system. 

Comparing with other optimal controllers such as LQR, MPC is more flexible and robust, and has a better adaption to model dynamics and different kinds of constraints. This makes MPC extremely popular in industry. The drawback of MPC is that it is more computationally demanding than LQR and other commonly seen optimal controllers.

The references of this chapter include \cite{rawlings2017model}.

\section{Introduction}

The MPC uses dynamic models to forecast the system trajectory, and substitute the it into the cost function. It looks for the optimal control sequence that minimizes the cost function. The first signal in the sequence is taken as the best decision at the current moment, and it is implemented to the actuator.

The following information is required to calculate the optimal control sequence:
\begin{itemize}
	\item The plant, which is used to forecast the system trajectory
	\begin{itemize}
		\item Model
		\item Current state
		\item Constraints for both state and control signal
	\end{itemize}
	\item Cost function
\end{itemize}
In practice, the model of the plant is usually calibrated by parameter estimation and the current state by state estimation. The constraints can be obtained from the equipment handbook. The cost function, usually in the form of a functional, is defined by the user.

\subsection{Model and Constraint}

The model plays an absolutely critical role in MPC. A model can be either linear or nonlinear, continuous or discrete, in state-space or input-output realizations, centralized or discrete (not spatially uniform), deterministic or stochastic. Different problem formulations and mathematical tools are used to describe the different types of the models and solve the different types of optimization problems.

MPC has a good adaption to different types of constraints. The constraints can be largely divided into two categories, namely physical constraints and performance constraints. Input signal $u(k)$ related constraints are often physical constraints reflecting the physical limits of the system. State vector $x(k)$ and output $y(k)$ related constraints, on the other hand, are often performance constraints. The performance constraints describes the lowest performance goals that the user expects from the system.

Input signal $u(k)$ related constraints are often given in the following form.
\begin{eqnarray}
	\left[\begin{array}{c}
		-I \\ I
	\end{array}\right]u(k) &\leq& \left[\begin{array}{c}
	-u_{\textup{min}} \\ u_{\textup{max}}
\end{array}\right] \label{eq:mpcconstraintu}
\end{eqnarray}

State vector $x(k)$ related constraints are often given in the following form.
\begin{eqnarray}
	Fx(k) &\leq& f \label{eq:mpcconstraintx}
\end{eqnarray}

It is possible to use augmented state vector that combines states with inputs. Let
\begin{eqnarray}
	\tilde{x}(k) &=& \left[\begin{array}{c}
	x(k) \\ u(k-1)
\end{array}\right] \nonumber
\end{eqnarray}
and both constraints in \eqref{eq:mpcconstraintu} and \eqref{eq:mpcconstraintx} can be expressed by the form
\begin{eqnarray}
	F\tilde{x}(k) + Eu(k) &\leq& e \label{eq:mpcconstraintux}
\end{eqnarray}
Equation \eqref{eq:mpcconstraintux} can also be used to limit the difference signal of the control signal. Consider
\begin{eqnarray}
	\left[\begin{array}{cc}
		0 & -I \\ 0 & I
	\end{array}\right]\tilde{x}(k) + \left[\begin{array}{c}
		I \\ -I
	\end{array}\right]u(k) &\leq& \left[\begin{array}{c}
		\Delta_{\textup{max}} \\ -\Delta_{\textup{min}}
	\end{array}\right] \nonumber
\end{eqnarray}
in \eqref{eq:mpcconstraintux}, which essentially translates to
\begin{eqnarray}
	\Delta_{\textup{min}} \leq u(k) - u(k-1) \leq \Delta_{\textup{max}} \nonumber
\end{eqnarray}

Other constraints include limiting the state vector and/or the control signal to be discrete values such as integers. These types of constraints are not as commonly seen as the aforementioned ones.

\subsection{Parameter Estimation and State Estimation}

The most commonly used parameter estimation method is the least squares estimation. The most commonly used state estimation method is the Kalman filter. Both least squares estimator and Kalman filter are linear filters and they are designed optimal for Gaussian noise. Least squares estimator can be implemented in a recursive manner, in which case it becomes a special case of Kalman filter mathematically.

It is possible to add ``forget factor'' to least squares estimator or Kalman filter. A window with fixed length is specified. When a new measurement set becomes available, the earliest measurement in the window will be removed, thus its impact on the state estimate eliminated. This leads to the moving horizon estimation (in contrast, the conventional Kalman filter implementation is also known as growing-memory estimation), which is less efficient but more robust to time-varying in the system.

\subsection{Optimal Control}

The optimal control to the plant vary with the definition of the cost function and constraints. In this section, for simplicity, a linear quadratic regulator problem is used to demonstrate MPC. Notice that although LQR is often regarded as a stand alone optimal control problem, there is no harm to introduce LQR as a simplified MPC problem, just for demonstration purpose.

\vspace{0.1in}
\noindent \textbf{Finite LQR Formulation}
\vspace{0.1in}

Consider the following dynamic model in discrete time domain. Current timestamp is $k=0$. For simplicity, it is assumed that the model is known and the state is measurable precisely, thus there is no need to build an estimator for the model or the state.
\begin{eqnarray}
  x(k+1) &=& Ax(k) + Bu(k) \label{eq:lqrexp1} \\
  y(k) &=& x(k) \label{eq:lqrexp2}
\end{eqnarray}

With the above model, it is possible to predict how the state evolves given a sequence of inputs as below.
\begin{eqnarray}
  u &=& \left[\begin{array}{cccc}
                u(0) & u(1) & \ldots & u(N-1) \nonumber
              \end{array}\right]
\end{eqnarray}
where $N$ is the largest time scale considered in this problem.

The purpose of the control is to regulate non-zero initial state $x(0)$ to zero while keeping the magnitude of $u(k)$ small. Therefore, define the cost function as follows.
\begin{eqnarray}
  V\left(x(0), u\right) &=& \dfrac{1}{2}\sum_{k=0}^{N-1}\left(x(k)^TQx(k) + u(k)^TRu(k)\right) + \dfrac{1}{2}x(N)^TP_fx(N) \nonumber \\ \label{eq:lqrexp3}
\end{eqnarray}
where $Q\geq 0$, $P_f \geq0$ and $R>0$ are symmetric matrices of user's choice. It can be proved that the above problem formulation must have a unique solution.

\vspace{0.1in}
\noindent \textbf{A Brief Introduction to Dynamic Programming}
\vspace{0.1in}

The aforementioned optimization problem in \eqref{eq:lqrexp3} can be solved via variety of ways. Consider using dynamic programming (DP).

A brief introduction to DP is given as follows. For illustration of DP, consider the following simple optimization problem.
\begin{eqnarray}
  x^*, y^* &=& \arg\min_{x,y} f(x, y) \label{eq:dpdemo1}
\end{eqnarray}
Since $x,y$ are coupled in $f(x,y)$, an intuitive way of solving this optimization problem is to let
\begin{eqnarray}
\left\{\begin{array}{ccc}
         \dfrac{\partial}{\partial x}f(x,y) & = & 0 \\
         \dfrac{\partial}{\partial y}f(x,y) & = & 0
       \end{array}\right. \nonumber
\end{eqnarray}
and if $f(x,y)$ is quadratics of $x, y$, the above should generate a unique solution.

Alternatively, consider solving \eqref{eq:dpdemo1} using the following approach. Think of \eqref{eq:dpdemo1} as a two-stage optimization problem. Given any constant value of $y$, there is an associated optimized $x^*(y)$ which minimizes \eqref{eq:dpdemo1}. Using this method, we can transform $f(x,y)$ into $f\left(x^*(y), y\right)$, a function of $y$ alone. From there, the optimal $y$ can be found easily. Subsequently, $x^*(y)$ can be obtained as well. To summarize, \eqref{eq:dpdemo1} is equivalent of
\begin{eqnarray}
y^* &=& \arg\min_{y}\left(\min_{x}f(x,y)\right) \nonumber \\
x^* &=& \arg\min_{x} f(x, y^*) \nonumber
\end{eqnarray}

Solving using DP as above can simplifies the problem, especially When only part of the optimal variable values is required (for example, only $y^*$ is required).

\vspace{0.1in}
\noindent \textbf{Solution to the Finite LQR Problem}
\vspace{0.1in}

The LQR problem in \eqref{eq:lqrexp3} is essentially something similar with \eqref{eq:dpdemo1}. The initial state $x(0)$ is a constant in the optimization problem, the control signals $u(0), ..., u(N-1)$ to be optimized, and the coupling introduced by the system dynamics \eqref{eq:lqrexp1}. The ultimate goal of the optimization problem is to determine $u(0)$.

Similarly, the optimization is divided into multiple stages, where in each stage only one instant $u(k)$ is considered, and other variables leading to that stage is considered as constant. We start with the last control input $u(N-1)$. The optimization problem is formulated as follows.
\begin{eqnarray}
  V^{\textup{sub}}(N-1) &=& \dfrac{1}{2}\left(x(N-1)^TQx(N-1) + u(N-1)^TRu(N-1)\right) \nonumber \\ && + \dfrac{1}{2}x(N)^TP_fx(N) \nonumber \\
  \textup{s.t.} && x(N) = Ax(N-1) + Bu(N-1) \nonumber
\end{eqnarray}
where $x(N-1)$ is taken as a constant from another stage.

Solving $u(N-1)$ as a function of $x(N-1)$ gives
\begin{eqnarray}
  u^*(N-1) &=& K_{N-1}x(N-1) \nonumber \\
  x(N) &=& (A+BK_{N-1})x(N-1) \nonumber \\
  V^{\textup{sub}}(N-1) &=& \dfrac{1}{2}x(N-1)^T\Pi_{N-1}x(N-1) \nonumber
\end{eqnarray}
where $K_{N-1}$ and $\Pi_{N-1}$ are two intermediate parameters calculated as follows.
\begin{eqnarray}
K_{N-1} &=& -(B^TP_fB + R)^{-1}B^TP_fA \nonumber \\
\Pi_{N-1} &=& Q + A^TP_fA-A^TP_fB(B^TP_fB+R)^{-1}B^TP_fA \nonumber
\end{eqnarray}

Consider the next stage optimization problem as follows.
\begin{eqnarray}
  V^{\textup{sub}}(N-2) &=& \dfrac{1}{2}\left(x(N-2)^TQx(N-2) + u(N-2)^TRu(N-2)\right) \nonumber \\
  \textup{s.t.} && x(N-1) = Ax(N-2) + Bu(N-2) \nonumber
\end{eqnarray}
where $x(N-2)$ is taken as constant and $u^*(N-2)$ is to be found. The solution is given by
\begin{eqnarray}
  u^*(N-2) &=& K_{N-2}x(N-2) \nonumber \\
  x(N-1) &=& (A+BK_{N-2})x(N-2) \nonumber \\
  V^{\textup{sub}}(N-2) &=& \dfrac{1}{2}x(N-2)^T\Pi_{N-2}x(N-2) \nonumber
\end{eqnarray}
where
\begin{eqnarray}
K_{N-2} &=& -(B^T\Pi_{N-1}B + R)^{-1}B^T\Pi_{N-1}A \nonumber \\
\Pi_{N-1} &=& Q + A^T\Pi_{N-1}A-A^T\Pi_{N-1}B(B^T\Pi_{N-1}B+R)^{-1}B^T\Pi_{N-1}A \nonumber
\end{eqnarray}

It is clear by now that the above calculations can be done recursively to obtain $u^*(0)$. Notice that all the control signals have the following form
\begin{eqnarray}
  u^*(k) &=& K_{k}x(k) \nonumber
\end{eqnarray}
which implies that the optimal control can be taken as a classic state-space feedback controller with time-varying control gains.

\vspace{0.1in}
\noindent \textbf{Infinite LQR Problem}
\vspace{0.1in}

Since the recursive calculation is done $N$ times to calculate $K_{N-1},...,K_0$, apparently different value of $N$ results in different $K_0$. It is worth mentioning that optimal control does not suggest stable control. In other words, it is possible for a specific system plant, specific choice of $Q$, $R$, $P_f$, and specific $N$, to lead to an unstable closed loop system, i.e., $A+BK_0$ with eigenvalues larger than 1.
However, it is guaranteed that when $N\rightarrow \infty$, $K_0$ converges, and $A+BK_0$ is always stable. This introduces the infinite LQR problem, where the cost function corresponding with \eqref{eq:lqrexp3} is changed to
\begin{eqnarray}
  V\left(x(0), u\right) &=& \dfrac{1}{2}\sum_{k=0}^{\infty}\left(x(k)^TQx(k) + u(k)^TRu(k)\right) \nonumber
\end{eqnarray}
Infinite LQR can be taken as a special case of finite LQR where $N\rightarrow\infty$.

We have been making an underlying assumption that the original plant is controllable. This means that the system can be driven from any initial state to $x=0$ in finite time. Apply this concept to infinite LQR problem. This implies that in the beginning finite steps of this infinite control sequence, the state should have already been regulated to zero.

Since the control gain $K(k), k=N-1, N-2, ...$ converges to $K$, the first infinite control gains would be $K$. Therefore, it can be concluded that the infinite LQR suggests the control gain to remain $K$ until all states becomes zero. Therefore, the infinite LQR problem results in constant control gain rather than time-varying control gains as in the finite LQR problem. The control gain can be obtained by solving the Riccati equation of $\Pi$ as follows.
\begin{eqnarray}
  K &=& -(B^T\Pi B + R)^{-1}B^T\Pi A \nonumber \\
  \Pi &=& Q + A^T\Pi A-A^T\Pi B(B^T\Pi B+R)^{-1}B^T\Pi A \nonumber
\end{eqnarray}

\subsection{Commonly Seen MPC Objectives}

The most commonly seen MPC objectives are tracking, state regulation, and disturbance rejection. 
\begin{itemize}
  \item Tracking: the output of the plant shall follow the given reference point (not necessarily to be a constant) as close as possible.
  \item State regulation: the state vector of the plant shall converge to a given steady-state as soon as possible, while not violating system restrictions.
  \item Disturbance rejection: the plant shall maintain its operating point under unknown stochastic noise and disturbances.
\end{itemize}








