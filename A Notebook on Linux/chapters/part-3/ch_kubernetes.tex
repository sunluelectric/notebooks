\chapter{Kubernetes}

Kubernetes is one of the most widely used container orchestration tools. Many cloud platforms provide Kubernetes support. Many opponent container orchestration tools are built on top of Kubernetes.

\section{Kubernetes Basics}

\mync{Kubernetes}, also known as \mync{k8s}, is an open-source container orchestration system originally developed by Google. It automates the deployment, health screening and scaling of containers for containerized applications.

As an orchestration tool, Kubernetes mainly focuses on the following tasks:
\begin{itemize}
	\item Monitor the status of the containers, and restart / replace broken ones.
	\item Balance the load assigned to the containers.
	\item Strategically scale up and down the number of containers based on the total load.
\end{itemize}
From the above, Kubernetes can be seen as a more powerful practice of what docker compose tries to do. It comes with additional features such as the support on multi-server deployment.

Managed services such as ECS can also do container orchestration. However, these managed services are often proprietary and inclusive to a particular cloud service provider. With such proprietary services, it is difficult to migrate the applications across platforms or to local premises. Kubernetes, on the other hand, is open-source and platform-independent, hence would not pose that problem.

Nowadays, many cloud service providers also support Kubernetes-based container deployment solutions such as \myabb{Elastic Kubernetes Service}{EKS} by AWS and \myabb{Google Kubernetes Engine}{GKE} by Google Cloud. They are also managed services to some extend and each platform usually have some unique features, but in general the application can be migrated across platforms without too much difficulty.

\begin{shortbox}
	\Boxhead{Kubernetes and Docker Engine}
	
	Kubernetes was built on top of docker engine. It used a special program \verb|dockershim| to talk to the underlying docker engine. Recently, however, docker support has been deprecated in Kubernetes. Consequently, \verb|dockershim| has also been removed. Now Kubernetes directly talks with container runtimes.
\end{shortbox}


\subsection{Infrastructure}

Figure \ref{ch:vac:fig:kubernetescluster} demonstrates the key components Kubernetes has inside its cluster.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=250pt]{chapters/part-3/figures/k8sarchitecture.png}
	\caption{Kubernetes cluster and its key components.} \label{ch:vac:fig:kubernetescluster}
\end{figure}
As shown in Fig. \ref{ch:vac:fig:kubernetescluster}, Kubernetes manages containers in a centralized ``master-worker'' manner, where the \mync{master node} (also known as \mync{control plane}) in yellow interacts with the user and schedules the pods to be deployed for each work node. The \mync{work nodes} (also known as \textbf{nodes}, for short) in green, on the other hand, process the application data. The arrows in Fig. \ref{ch:vac:fig:kubernetescluster} represents the control flow. Notice that the application data flows differently and it does not pass through the master. There can be multiple master nodes (in replica mode) and worker nodes, though only one of each is shown in Fig. \ref{ch:vac:fig:kubernetescluster}.

Key components in the master and the nodes and their functions are summarized in Table \ref{ch:vac:tab:keycomponents}.

\begin{table}[!htb]
	\centering
	\caption{Key components in Kubernetes master and nodes.} \label{ch:vac:tab:keycomponents}
	\begin{tabularx}{\textwidth}{llX}
		\hline
		Component & Location & Description \\
		\hline
		\verb|etcd| & Master & A distributed key-value pair database that stores the status of the Kubernetes cluster. The consistency of the distributed database is achieved based on Raft consensus algorithm. \\ \hline
		API server & Master & The gateway to handle all interactions with the Kubernetes cluster. For example, \verb|kubectl| requests from the user are handled by API server. \\ \hline
		Scheduler & Master & The optimizer that calculates the desired pod distributions among all nodes based on the total load and the hardware limits. Notice that scheduler is not involved into load balance directly. A separate load balancer service can be created to actually direct the traffic. \\ \hline
		Controller Manager & Master & A set of controllers or control functions to reconcile the actual Kubernetes cluster status with what has been calculated by the scheduler and stored in \verb|etcd|. \\ \hline
		Kubelet & Node & Local controller for the node. It ensures that all the pods assigned to the node are running, and it keeps listening to the master for further instructions. \\ \hline
		Kube Proxy & Node & The internet service provider in the node. \\
		\hline
	\end{tabularx}
\end{table}

Each node can host multiple pods. A pod object is a single container or a collection of containers for a single task, and it is also the smallest unit that Kubernetes controls directly. Notice that in Kubernetes, containers never run directly in a node. They are always grouped into pods. More details about the pod object are introduced in later Section \ref{ch:vac:sec:objects}.

Both master and nodes can run on multiple servers or VMs. For master, multiple master replicas can run in parallel to take care of user requests and provide database services. The ``decision makers'', i.e. the controller manager and scheduler, run only on one of the master nodes which is elected via a leader election process. Should the decision maker master fail, one of the replicas will be promoted. For nodes, multiple nodes can run together to precess information in parallel and Kubernetes manages the distribution of load among them.

It is worth mentioning that many services cluster-level services such as ClusterIP, NodePort, LoadBalancer and Ingress Controllers, though being cluster-level, run in worker nodes but not in the master. 

\subsection{Installation}

Kubernetes is not a single piece of software but more an architecture that involves many pieces of software such as:
\begin{itemize}
  \item \verb|kubectl| the CLI
  \item \verb|kube-apiserver| the API server
  \item \verb|etcd| the distributed key-value pair database
  \item \verb|kube-scheduler| the scheduler
  \item \verb|kube-controller-manager| the control manager
  \item Kubelet
  \item Kube Proxy
  \item Kubernetes-supported container runtime such as \verb|containerd|
  \item Container Network Interface (CNI) plugin such as Calico
  \item \ldots
\end{itemize}
The installation guidance of Kubernetes can be found at its official website \textit{kubernetes.io}. In a production environment, the administrator shall install and configure the above tools on the servers by himself. This gives him the flexibility of customizing the tools. However, the process can be tedious and requires experience.

In this notebook, for demonstration purpose, we will use Minikube to help build a Kubernetes cluster. Minikube is an open-source software developed by the Kubernetes community to start a VM and run a single-node Kubernetes cluster on a local machine. It automates most of the tools installations and configurations which is in favour of the scope of this notebook. 

We still need to install Minikube and also \verb|kubectl|. As introduced earlier, \verb|kubectl| is used to interact with Kubernetes clusters deployed either locally or remotely. It is always recommended to have \verb|kubectl| installed on the local machine wherever the Kubernetes cluster is deployed. The installation of \verb|kubectl| can be found at Kubernetes website at
\begin{lstlisting}
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/
\end{lstlisting}
Minikube installation can be found at
\begin{lstlisting}
https://minikube.sigs.k8s.io/docs/start/
\end{lstlisting}

Once Minikube is installed, start it using
\begin{lstlisting}
$ minikube start
\end{lstlisting}
When starting Minikube, it automatically detects which container engine to use, and some configurations might be required subsequently.

When running Minikube, it uses existing VM engines such as VirtualBox, or container engines such as docker and podman installed on the local machine to start a VM or container that hosts Kubernetes. Therefore, it will need to execute the VM tools or the container engines. In naive RHEL, Minikube needs to execute podman. Some of the commands Minikube need to run require sudo privilege, and as a result, it requires \verb|NOPASSWD| configuration to start the Kubernetes cluster correctly. 

This can be done as follows. Open the sudoer configuration file by
\begin{lstlisting}
$ sudo visudo
\end{lstlisting}
and append
\begin{lstlisting}
<user name> ALL=(ALL) NOPASSWD: /usr/bin/podman
\end{lstlisting}
to the file.

Once Minikube is started, \verb|kubectl| installed on the local machine should be able to detect the Kubernetes running inside. Sse the following command to verify that \verb|kubectl| has detected and connected with the Kubernetes runtime.
\begin{lstlisting}
$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
\end{lstlisting}

\begin{shortbox}
\Boxhead{Minikube's Built-in \texttt{kubectl}}

Minikube also comes with a built-in \verb|kubectl|. It is possible to use that \verb|kubectl| instead of installing one separately. To very the existence of the built-in \verb|kubectl|, start Minikube first and then use

\begin{lstlisting}
$ minikube kubectl -- cluster-info
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
\end{lstlisting}
where \verb|--| in the command tells the shell that all the followed arguments shall be passed to \verb|kubectl| inside Minikube, but not \verb|minikube| itself. For convenience, one may want to use alias as follows
\begin{lstlisting}
$ alias kubectl="minikube kubectl --"
\end{lstlisting}

With the above been said, it is still recommended that \verb|kubectl| to be installed on the local machine, not just in the VM. 
\end{shortbox}

It is possible to run Kubernetes cluster directly on a host machine OS without VM if that machine is running Linux. However, this is not recommended for reasons pertaining to access control, security, and isolation. It is generally a better practice to deploy Kubernetes clusters in VMs.

The configuration file for \verb|kubectl| is usually located at \verb|~/.kube/config|. It determines the basic setups of the \verb|kubectl| instance, such as which Kubernetes cluster \verb|kubectl| communicates with.

\subsection{Kubernetes Cluster Manipulation}

There are at least two ways for a user to interact with a Kubernetes cluster and deploy or manipulate services in it.
\begin{itemize}
  \item Imperative approach: use \verb|kubectl| commands to directly start, change or stop pods and services.
  \item Declarative approach: prepare Kubernetes configuration files that describes the ``desired state'' of the Kubernetes cluster, and use \verb|kubectl -f| command to implement the files. Kubernetes decides how to deploy pods on servers to achieve the desired state.
\end{itemize}
Both approaches are introduced as follows. 

\vspace{0.1in}
\noindent \textbf{Cluster Deployment with Imperative Approach}
\vspace{0.1in}

The imperative approach, in some sense, destroys the purpose of Kubernetes as a container orchestration tool since the user decides the deployment of pods and services manually. On the other hand, the declarative approach takes advantage of the full capability of Kubernetes and it is usually considered a better practice. Yet, learning the imperative approach helps the user with getting familiar with \verb|kubectl| commands. In this section, imperative approach and its relevant generally used \verb|kubectl| commands are briefly introduced.

Use the following to view the entire list of \verb|kubectl| supported commands.
\begin{lstlisting}
$ kubectl help
kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/

Basic Commands (Beginner):
  create          Create a resource from a file or from stdin
  expose          Take a replication controller, service, deployment or pod and expose it as a new Kubernetes service
  run             Run a particular image on the cluster
  set             Set specific features on objects

Basic Commands (Intermediate):
  explain         Get documentation for a resource
  get             Display one or many resources
  edit            Edit a resource on the server
  delete          Delete resources by file names, stdin, resources and names, or by resources and label selector

Deploy Commands:
  rollout         Manage the rollout of a resource
  scale           Set a new size for a deployment, replica set, or replication controller
  autoscale       Auto-scale a deployment, replica set, stateful set, or replication controller

Cluster Management Commands:
  certificate     Modify certificate resources
  cluster-info    Display cluster information
  top             Display resource (CPU/memory) usage
  cordon          Mark node as unschedulable
  uncordon        Mark node as schedulable
  drain           Drain node in preparation for maintenance
  taint           Update the taints on one or more nodes

Troubleshooting and Debugging Commands:
  describe        Show details of a specific resource or group of resources
  logs            Print the logs for a container in a pod
  attach          Attach to a running container
  exec            Execute a command in a container
  port-forward    Forward one or more local ports to a pod
  proxy           Run a proxy to the Kubernetes API server
  cp              Copy files and directories to and from containers
  auth            Inspect authorization
  debug           Create debugging sessions for troubleshooting workloads and nodes
  events          List events

Advanced Commands:
  diff            Diff the live version against a would-be applied version
  apply           Apply a configuration to a resource by file name or stdin
  patch           Update fields of a resource
  replace         Replace a resource by file name or stdin
  wait            Experimental: Wait for a specific condition on one or many resources
  kustomize       Build a kustomization target from a directory or URL

Settings Commands:
  label           Update the labels on a resource
  annotate        Update the annotations on a resource
  completion      Output shell completion code for the specified shell (bash, zsh, fish, or powershell)

Subcommands provided by plugins:

Other Commands:
  api-resources   Print the supported API resources on the server
  api-versions    Print the supported API versions on the server, in the form of "group/version"
  config          Modify kubeconfig files
  plugin          Provides utilities for interacting with plugins
  version         Print the client and server version information

Usage:
  kubectl [flags] [options]
\end{lstlisting}

Use
\begin{lstlisting}
$ kubectl create <object type> <object name> --<argument>
\end{lstlisting}
to create and deploy an object, and use
\begin{lstlisting}
$ kubectl get <object type>
\end{lstlisting}
to view deployed objects of certain type. Notice that in \verb|kubectl get| is usually followed by the plural form of the object type. An example is given below.
\begin{lstlisting}
$ kubectl create deployment <deployment name> --image=<image name>
$ kubectl get deployments
\end{lstlisting}
Deployment object is deployed in the above example. More about Deployment object will be given in later Section \ref{ch:vac:sec:objects}. It is one of the most commonly used Kubernetes objects and serves well as an example to illustrate imperative approach. Notice that the image assigned to the deployment, in the above example \texttt{hello-world}, needs to be reachable by Kubernetes. In this example, the image is to be deployed by the Kubernetes cluster deployed in the Minikube environment, and hence any image locally built in the host machine will not be reachable. The image has to be stored on an online registry.

The deployed pods will be default have a cluster-level internal IP. There are the following problems with this internal IP: it is dynamic, and it is not accessible from outside the cluster. To expose the IP and port, a separate Service object needs to be used as follows.
\begin{lstlisting}
$ kubectl expose deployment <deployment name> --type=<service type>
\end{lstlisting}
where Service object type can be \verb|ClusterIP|, \verb|NodePort|, \verb|LoadBalancer|, etc. More about Service object will be given in later Section \ref{ch:vac:sec:objects}.

To scale up and down the number of pods in a Deployment object, use
\begin{lstlisting}
$ kubectl scale --replicas=<number> deployment <deployment name>
\end{lstlisting}

And finally to update the image of the deployment, use
\begin{lstlisting}
$ kubectl set image deployment/<deployment name> <container name>=<image name: tag>
\end{lstlisting}
For example,
\begin{lstlisting}
$ kubectl set image deployment/nginx-deployment my-nginx-container=nginx:1.25.1
\end{lstlisting}

The user can check the status of the roll out by
\begin{lstlisting}
$ kubectl rollout status deployment/<deployment name>
\end{lstlisting}
When Kubernetes is rolling out a new version, the old version will not be shutdown. This minimizes the blackout time when an application is being updated.

The user can check the history of the roll out using
\begin{lstlisting}
$ kubectl rollout history deployment/<deployment name> [--revision=<revision index>]
\end{lstlisting}
where \verb|[--rivision]| will provides details to the roll out.

To roll back to an earlier version in the history, use
\begin{lstlisting}
$ kubectl rollout undo deployment/<deployment name> [--to-revision=<revision index>]
\end{lstlisting}
where if \verb|--to-revision| is not used, the command rolls back to the earlier latest version.

\vspace{0.1in}
\noindent \textbf{Cluster Deployment with Declarative Approach}
\vspace{0.1in}

Kubernetes configuration files describe the desired final status of the Kubernetes cluster. Should these files be passed to Kubernetes, it should be able to determine the processes to deploy relevant Kubernetes objects and eventually realize and maintain the desired state. This is known as the declarative approach and it is the default and recommended way Kubernetes shall be used. The declarative approach has at least the following advantages over the imperative approach:
\begin{itemize}
  \item It automates the Kubernetes deploying by using configuration files instead of the user running \verb|kubectl| commands manually, which simplifies the process and reduces human error.
  \item It leverages on the container orchestration capability of Kubernetes.
\end{itemize}
These advantages are critical especially in large-scale projects when there are multiple Kubernetes objects in the cluster.

The details of Kubernetes objects and Kubernetes configuration files composing are introduced in later Sections \ref{ch:vac:sec:objects} and \ref{ch:vac:sec:kubernetesconfiguration}. In this section, a simple example is used to illustrate the basic commands for the declarative approach. Below is an example from the Kubernetes website. It contains the configuration of two objects.
\begin{lstlisting}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
\end{lstlisting}

Some commonly used fields used in the Kubernetes configuration files are briefly summarized below.
\begin{itemize}
	\item \verb|apiVersion|: The API version of the object. Different object types are supported in different API versions.
	\item \verb|kind|: The object type.
	\item \verb|metadata|: The name and labels of the object. 
	\item \verb|spec|: The specifications of the object. Different object types require different specifications.
\end{itemize}
More details of the configuration file composing of each type of Kubernetes object are introduced in later Section \ref{ch:vac:sec:objects}. 

With the image and the configuration files ready, the next step is to deploy the nodes, pods, and containers. The \verb|kubectl| CLI is used to instruct Kubernetes to deploy the objects as follows.
\begin{lstlisting}
$ kubectl apply -f <configuration file> / <files directory>
\end{lstlisting}
This essentially asks the master node in the Kubernetes cluster to start taking actions following the Kubernetes configuration files, such as to inform the nodes to start creating pods and containers. The master node also keeps monitoring the status of each work node, to make sure that everything is running as planned. If there is a container failure, etc., the master node will guide the associated note to restart the container.

It is possible to consolidate the configuration files of objects into one configuration file. To do that, use \verb|---| to split the configurations for each object in the conjunctive configuration file as follows. It is of personal preference whether to use conjunctive configuration files or separate configuration files for all objects.
\begin{lstlisting}
<configurations for object 1>
---
<configurations for object 2>
---
<...>
\end{lstlisting}
The order of the configurations appears in the file usually does not matter. However, it is often considered a better practice to put cluster-level objects such as Service objects in front of work split objects such as Development objects.

With the help of Kubernetes declarative approach, it is possible to update the cluster simply by revising the configuration files, and pass them to Kubernetes as if the cluster is to be deployed for the first time. Kubernetes automatically checks revised configuration files, comparing them with existing running objects, and update them if necessary.

It is recommended to check the status of the pods using \verb|kubectl get pods| once the cluster is updated. After the update, the pods should be restarted, and hence the ``RESTARTS'' tag shall increase. To ensure that the update is successful, use \verb|kubectl describe| to check the details of the relevant objects.

Notice that there is a limitation on what can be updated to an existing Kubernetes deployment. For an existing object, only certain fields in the Kubernetes configuration files are allowed to be revised. For example, for a pod, the image can be changed while the ports cannot. Some objects types are more flexible than others when comes to object updating. The user shall choose wisely what object types to use considering what updates need to be made in future.

To revert what has been applied in a Kubernetes configuration file, use
\begin{lstlisting}
$ kubectl delete -f <configuration file>
\end{lstlisting}
The user can also delete certain types of objects by labels by
\begin{lstlisting}
$ kubectl delete <object type 1>[, <object type 2>, ...] -l <label>
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Kubernetes Cluster Status Check}
\vspace{0.1in}

To retrieve the status of a group of objects, use
\begin{lstlisting}
$ kubectl get <object type>
\end{lstlisting}
where \verb|<object type>| can be \verb|pods|, \verb|services|, etc. For more details of a specific object, use
\begin{lstlisting}
$ kubectl describe <object type> <object name>
\end{lstlisting}
for example, to check the containers running in a pod. If \verb|<object name>| is neglected, Kubernetes returns detailed information of all objects of the given object type. For a running object, use
\begin{lstlisting}
$ kubectl logs <object name>
\end{lstlisting}
to check the log file of that object.

Minikube provides a web-based dashboard where the user can conveniently check the status of the cluster. Use
\begin{lstlisting}
$ minikube dashboard
\end{lstlisting} 
to start the dashboard and follow the instructions on the concole to open the dashboard. The dashboard gives the detailed information of running objects in the Kubernetes cluster, such as the healthy status of pods, the cluster-level internal IP of objects, etc.


Kubernetes treats the above as an update and will action accordingly.

\section{Kubernetes Objects} \label{ch:vac:sec:objects}

This section introduces commonly used Kubernetes objects, a list of which is given in Table \ref{ch:vac:tab:objtype}.

\begin{table}[!htb]
	\centering
	\caption{Commonly used Kubernetes object types.} \label{ch:vac:tab:objtype}
	\begin{tabularx}{\textwidth}{lX}
		\hline
		Object Type & Description \\
		\hline
		Pod & Represents a single instance of a process running in the cluster. A pod contains one or multiple containers that work closely to deliver a basic function. It is the smallest unit of process in Kubernetes. \\ \hline
		Deployment & Manages the deployment and scaling of a set of identical pods, ensuring the desired number of replicas are running and providing rolling updates for seamless application upgrades. \\ \hline
		Service & Enables network access to the node or to a set of pods using a stable IP address and DNS name. It provides load balancing across multiple pod replicas and allows external traffic to be directed to the appropriate pods. \\ \hline
		ConfigMap & Stores configuration data in key-value pairs, which can be consumed by pods as environment variables, command-line arguments, or mounted as files. \\ \hline
		Secret & Similar to a ConfigMap, but specifically designed to store sensitive data, such as passwords, API keys, and TLS certificates. Secrets are encrypted at rest and can be mounted into pods as files or exposed as environment variables. \\ \hline
		PersistentVolume & Provides a way to provision and manage persistent storage resources in a cluster. It decouples the storage from the underlying infrastructure and allows data to persist beyond the lifecycle of individual pods. \\ \hline
		PersistentVolumeClaim & Requests a specific amount of storage from a PersistentVolume. It acts as a request for a specific storage resource and provides an abstraction layer for managing persistent storage in a cluster. \\ \hline
		Ingress & Manages external access to services within a cluster. It acts as a reverse proxy and exposes HTTP and HTTPS routes to route traffic to the appropriate services based on hostnames, paths, or other rules. \\
		\hline
	\end{tabularx}
\end{table}

These object types are introduced in more details in the remaining of the section.

\subsection{Pod Object}

\mync{Pod} object is the smallest and most fundamental unit in a node that Kubernetes interacts with. A pod is essentially a ``wrap'' that hosts one or multiple containers that work closely with each other. Pod contains shared resources such as volumes for all the containers it hosts. The containers in a pod can also communicate each other via \verb|localhost|. A pod has a cluster-internal IP by default.

Just like a container, pods are designed to be ephemeral. Kubernetes can start and stop them depending on the load and the traffic coming into the cluster, and when they are removed, their data vanishes. Of course there are many ways to persist the data, for example by directing the data into a volume or a database.

The user can deploy pods using either \verb|kubectl| command imperatively or in Kubernetes configuration files declaratively. However, the user would not do so in practice, as it destroys the purpose of using Kubernetes.

A pod is assigned with a cluster-level internal IP by default, and that IP can be used for communications inside the Kubernetes cluster. It has the following problems though, making the IP address difficult to use:
\begin{itemize}
  \item The IP address is internal the cluster, hence useless to entities outside the container.
  \item Even for communications inside the cluster, the IP address is not convenient to use since it is dynamic.
\end{itemize}

\subsection{Deployment Object}

It is more often that the user would want to deploy Deployment objects instead of pods. A \mync{Deployment} object serves as the ``controller'' of one or a group of identical pods, and it serves as a good example to illustrate declarative approach where the user specifies a desired state and the Deployment object carries out the necessary procedures to realize that state. A Deployment object can do at least the following for the pods it manages:
\begin{itemize}
  \item Start and stop the pods strategically at different word nodes.
  \item Monitor the healthy status of the pods and strategically restart them when necessary.
  \item Scale up and down the number of pods.
  \item Balance the loads that go into each pod.
  \item Update or roll back the pods.
\end{itemize}

As an example, a Kubernetes cluster file that deploys an Deployment object is given below. Notice that Deployment object is supported in API version \verb|apps/v1|.
\begin{lstlisting}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80

\end{lstlisting}
Some highlights under \verb|spec| are introduced below.
\begin{itemize}
	\item \verb|replicas|: the number of pods under the Deployment object should maintain. The value can be zero if no pods need to be deployed initially.
	\item \verb|selector.matchLabels|: a set of labels, which if a pod possesses, will be monitored and likely managed by the Deployment object.
	\item \verb|template|: the template that describes the pods this Deployment object creates, including:
    \begin{itemize}
      \item \verb|metadata.labels|: the labels the pods will inherit.
      \item \verb|spec.containers|: the containers details, such as name, image, port, etc.
    \end{itemize}
\end{itemize}
Notice that a Deployment object can deploy only identical pods from the same template. If different types of pods need to be deployed, different Deployment objects are required. However, under one pod, there can be multiple containers populated from different images. Notice that \texttt{template.spec.containers} contains a list of containers, each with its name, image, ports, etc. Append that list if multiple containers are required under the same pod.

An alternative to \texttt{selector.matchLabels} is \texttt{selector.matchExpressions}, the later of which provides a more powerful and flexible way of mapping labels. More details of \texttt{selector.matchExpressions} is not introduced in this notebook, as \texttt{selector.matchLabels} should serve well in most scenarios.

\begin{shortbox}
\Boxhead{Labels in \texttt{selector} and \texttt{template}}

It may feel redundant that the same labels appear twice under both \texttt{selector.matchLabels} and \texttt{template.metadata.labels}. This sense of redundancy often stems from the assumption that a Deployment object should watch (and only watch) the pods it creates.

In reality, multiple controllers can ``monitor'' the same pods, potentially for metrics, auditing, or other read-only operations. A Pod has exactly one owner which is the controller that creates the pod, but it can be monitored by other controllers or tools via label selector.

By splitting \texttt{selector.matchLabels} and \texttt{template.metadata.labels} instead of automatically inheriting one from the other, Kubernetes provides flexibility in how pods and Deployment objects are architected. Additional labels can be added in the template to support other purposes—like monitoring, cost allocation or environment tagging without affecting the Deployment’s fundamental ownership of those Pods.

\end{shortbox}

When a new version of an image becomes available, we may want to update the containers accordingly. Re-apply the same configuration file would not help, as Kubernetes would reject apply request if no change is detected in the configuration file. It would not check whether the image is in its latest version. The following imperative command which has been introduced earlier can be used to update the image.
\begin{lstlisting}
$ kubectl set image deployment/<deployment name> <container name>=<image name: tag>
\end{lstlisting}
The image in the specified container will be updated. Notice that the new image will be downloaded only if it has a different tag. This encourages the developer to update the tag formally when publishing images.

\subsection{Service Object} \label{ch:vac:subsec:k8snetworking}

A \mync{Service} object is used to build network services in a Kubernetes cluster. As introduced earlier, though objects such as pods will be automatically assigned with cluster-level internal IPs, they are close to useless in practice. Service objects can be used to build a more useful and robust network infrastructure for a Kubernetes cluster.

There are 4 subtypes of Service objects, including
\begin{itemize}
	\item ClusterIP: a ClusterIP Service object is assigned with a static cluster-level internal IP which is useful for communications of services within the cluster.
	\item NodePort: a NodePort Service object exposes a port on all the work nodes, using which outside entities can communicate with the NodePort Service object and then talk to objects inside the Kubernetes cluster. Notice that NodePort is often used in development environment and not production environment, in which case there are other preferable solutions such as LoadBalancer.
	\item LoadBalancer: a LoadBalancer Service object is essentially a NodePort Service object integrated with an externally provisioned load balancer. Notice that Kubernetes does not ship with this load balancer, but rather provides an interface that connects to the external load balancer often provided by the cloud service provider.
	\item ExternalName: an ExternalName Service object instructs Kubernetes to create a DNS alias within the cluster’s DNS. This lets Pods in the cluster use a short, consistent service name to reach an external resource.
\end{itemize}

A Service object configuration file may look like the following.
\begin{lstlisting}
apiVersion: v1
kind: Service
metadata:
  name: client-cluster-ip-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: 'TCP'
      port: <port>
      targetPort: <port>
  type: ClusterIP # ClusterIP, NodePort or LoadBalancer
\end{lstlisting}
Some highlights are introduced below.
\begin{itemize}
  \item \verb|spec.selector|: a list of labels with which the objects can connect to this Service object.
  \item \verb|spec.ports.port|: port exposed by the ClusterIP Service object to the cluster.
  \item \verb|spec.ports.targetPort|: port exposed by the containers to the ClusterIP Service object.
\end{itemize}

\subsection{Ingress Object}

Ingress is a more commonly used Service type than LoadBalancer to get traffic into the Kubernetes containers. There are different types of Ingress, for example, Nginx Ingress by \textit{github.com/kubernetes/ingress-nginx}. A demonstrative example of ingress service realization is given in Fig. \ref{ch:vac:fig:ingress_service}. In this implementation framework, the configuration file (mainly a set of routing rules) of the object is used to define an ``Ingress Controller'' which manages the runtime that controls inbound traffic. In some applications such as \textit{kubernetes/ingress-nginx}, the ingress controller and the runtime are integrated together.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=350pt]{chapters/part-3/figures/ingress_service.png}
	\caption{An example of ingress service framework.} \label{ch:vac:fig:ingress_service}
\end{figure}

The ingress configuration differs depending on the ingress service type and the platform. Details are not given here.

\subsection{Volume Object} \label{ch:vac:subsec:k8svolume}

Volumes are handy tools to persist data in containers so that it would not vanish when the container is terminated. Container volumes are used to persist data for locally running containers managed by container engines and the details have been introduced in Section \ref{ch:vac:subsec:dockervolume}.

For similar purpose, Kubernetes introduces Volume object to persist state and data of containers. The Volume object will then add container volumes as desired, and attach them to the specified containers. More details are introduced as follows.

There are three types of Volume objects as follows.
\begin{itemize}
  \item Kubernetes Volume: A volume tied to the pod. It survives container failure, but will not survive if the pod fails or being removed.
  \item Kubernetes \mync{Persistent Volume}[PV]: A volume tied to the host machine. It survives even if the Kubernetes cluster is stopped.
  \item Kubernetes \mync{Persistent Volume Claim}[PVC]: It is a ``claim'' or ``request'' of PV initiated from a pod or a container. Kubernetes will try to map PVC with existing PV, or dynamically provision PV for the claim.
\end{itemize} 

Docker engine uses volumes to maintain persistent data and share data among containers. Details have been introduced in Section \ref{ch:vac:subsec:dockervolume}. Kubernetes volume framework is similar in the sense that it makes sure that the data is saved and managed by the host machine, so that when the pods or containers are shutdown or restarted, the data can be restored safely.

An example of claiming Kubernetes PV and PVC is given below. In the remaining part of this section, we will be mostly using PVC instead of PV.
\begin{lstlisting}
# persistent-volume.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
name: my-pv
spec:
storageClassName: standard
capacity:
storage: 10Gi
accessModes:
- ReadWriteOnce
hostPath:
path: /data/my-pv
---
# persistent-volume-claim.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: my-pvc
spec:
storageClassName: standard
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 5Gi
\end{lstlisting}
To check the PV objects, use \verb|$ kubectl get pv| and \verb|$ kubectl get pvc|.

To add the above Kubernetes PVC to a Kubernetes Deployment, add volumes information to the specs as given in the following example.
\begin{lstlisting}
apiVersion: apps/v1
kind: Deployment
metadata:
name: my-app-deployment
spec:
replicas: 1
selector:
matchLabels:
app: my-app
template:
metadata:
labels:
app: my-app
spec:
containers:
- name: my-app-container
image: my-app-image
ports:
- containerPort: 8080
volumeMounts:
- name: data-volume
mountPath: /data
subPath: data-from-container
volumes:
- name: data-volume
persistentVolumeClaim:
claimName: my-pvc
\end{lstlisting}
where \verb|volumes| defines which Kubernetes PVC is used, and \verb|volumeMounts| tells how it is mounted in the container. The \verb|mountPath| is the path in the container whose data is mounted by the volume. If a \verb|subPath| is given, a sub-folder of its specified name will be created in the host machine in the volume to contain the data.

There are different types of access modes:
\begin{itemize}
	\item \verb|ReadWriteOnce|: Allow one node to read and write at a time.
	\item \verb|ReadOnlyMany|: Allow many nodes to read at a time.
	\item \verb|ReadWriteMany|: Allow many nodes to read and write at a time.
\end{itemize}

The developer can specify the place for Kubernetes PVs. This is usually the hard drive on a local server, a virtual storage space in the VM. Use the following command to check Kubernetes possible choice of storage.
\begin{lstlisting}
$ kubectl get storageclass
\end{lstlisting}
and
\begin{lstlisting}
$ kubectl describe storageclass
\end{lstlisting}
When deploying Kubernetes on the Cloud, the developer needs to do additional configurations as there would be many storage options. Usually, each Cloud provider will have its own default storage space for Kubernetes, such as AWS Elastic Block Store for AWS.

\subsection{Secrets} \label{ch:vac:subsec:k8ssecrets}

Kubernetes Secrets object is used store confidential information, such as the database password, API key, etc. It is often a piece of information that is necessary for the containers, but the developer does not want to present as plain text in the configuration file.

Secrets are not created from configuration files, which is the recommended way of creating other Kubernetes objects. Instead, it is created from one-time imperative command, inside which the confidential information needs to be told to Kubernetes. Use the following command to create a Secret object.
\begin{lstlisting}
$ kubectl create secret <type-of-secret> <secret-name> --from-literal <key>=<value>
\end{lstlisting}
There are 3 types of Secrets: \verb|generic|, \verb|docker-registry| and \verb|tls|.

\section{Advanced Kubernetes Cluster Configuration} \label{ch:vac:sec:kubernetesconfiguration}

This section discusses in more details the composing of the Kubernetes cluster configuration files.

\subsection{A Glance of Useful Configurations}

The more familiar the user is with the supported configurations of each Kubernetes object, the more likely he can leverage on the maximum capability of Kubernetes cluster. To this point, the basic configurations of commonly seen Kubernetes objects, such as \verb|spec.containers| for Deployment objects and \verb|spec.ports| for Service objects, have been introduced. This section discusses some more advanced configurations that the user may find useful in practice.

The \verb|livenessProbe| under \verb|spec.containers| of Deployment objects allows the user to define how the healthiness of the container shall be monitored. For example, the user can define a watch dog by asking the container to send an HTTP get request to certain port at certain frequency, and so on. Notice that Kubernetes has default liveness probe mechanism. Still, the flexibility where the user can override it with a custom probe is appreciated.

By default, the Deployment object will pull the images when
\begin{itemize}
  \item The image is not saved in the locally, in which case Kubernetes has to pull the image.
  \item The \verb|:latest| tag is used with the image, in which case Kubernetes will pull the image each time before launching the containers.
\end{itemize}
The \texttt{spec.containers.imagePullPolicy} of Deployment objects can be used to change the policy of pulling the image. For example, if \texttt{imagePullPolicy: Always} is used, the Deployment project will always pull the image before starting the containers.

\subsection{Environmental Variables}

Kubernetes environment variables are used to pass or share information among Deployments. Depending on the features of the information, such as whether it is a constant global configuration or a dynamic value, whether it is plain text or confidential encoding, etc., it might be handled differently.

To define constant environment variables in containers, simply specify them in the Deployment configuration file as given in the example below.

\begin{lstlisting}
apiVersion: apps/v1
kind: Deployment
metadata:
name: my-app-deployment
spec:
replicas: 1
selector:
matchLabels:
app: my-app
template:
metadata:
labels:
app: my-app
spec:
containers:
- name: my-app-container
image: my-app-image
ports:
- containerPort: 8080
volumeMounts:
- name: data-volume
mountPath: /data
subPath: data-from-container
env:
- name: <name1>
value: <value1>
- name: <name2>
value: <value2>
- name: <name3>
valueFrom:
secretKeyRef:
name: <secret-name>
key: <key>
volumes:
- name: data-volume
persistentVolumeClaim:
claimName: my-pvc
\end{lstlisting}
where a new tag \verb|env| is defined under template, specifications, containers. Under the \verb|env| tag, a list is defined containing names and values of the environment variables. The value must be a string, not a numerical number.

\section{Kubernetes Deployment in Production Environment}

With the tools and methodologies introduced so far, we are able to deploy containers in development environment. This is good enough for testing purpose or for small individual projects. However, when comes to enterprise tier projects or collaborative projects, there is often a CI/CD pipeline that standardize the integration and delivery of the containers in production environment. Container orchestration tools such as Kubernetes is often a must have.

This section briefly introduces the steps to develop and deploy containers in production environment with Kubernetes. Figure \ref{ch:vac:fig:prodenvworkflow} gives an example of overview of what such deployment may look like. Notice that this example is more towards a community project but not an enterprise project.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=350pt]{chapters/part-3/figures/prodenvworkflow.png}
	\caption{An example workflow of creating a production environment with Kubernetes.} \label{ch:vac:fig:prodenvworkflow}
\end{figure}

The example used in this section to demonstrate Kubernetes CI/CD on a cloud platform in production environment is taken from \cite{stephen2023docker}. Following \cite{stephen2023docker}, Google Cloud Platform is used as the cloud platform provider.

\subsection{Setup Cloud Account}

Many cloud platforms nowadays have a very good support of Kubernetes. The deployment of containers using Kubernetes can be done via their UIs easily. The developer needs to decide the resources to be used for the deployment. The more nodes and more power machine, the higher the charge. In most cases, the developer does not need to start a VM and install Kubernetes on it by himself. The cloud provider shall have dedicated Kubernetes engine service that would automatically configure the VM per required.

\subsection{Configure CI/CD}

\myabb{Continuous integration and continuous delivery}{CI/CD} is a widely appreciated workflow in modern software development. More about CI/CD is given in Appendix \ref{ch:cicd}.

Travis CI is a continuous integration service tool written. It can be deployed on the cloud and linked to a Github repository and a CI/CD platform such as a Kubernetes cluster on Google Cloud Platform.

To run Travis, a machine supporting Ruby programming language is required. For that, a separate container with Ruby installation is deployed to run Travis. Use Github credentials to login to Travis, so that it can link to the Github repositories.

Travis has built-in file encryption function. This function is mainly used to encrypt login credentials and service account credentials (in this example, the service account information to link to Kubernetes clusters on Google Cloud Platform) locally, so that later the unencrypted original credential files can be deleted, and only the encrypted credentials uploaded to the Github repository. When encrypting a file, Travis will also guide the user on how to call the encrypt information in the build script.

Travis uses configuration files to setup CI/CD pipeline.

\subsection{Deploy Containers}

In the Travis configuration file, \verb|deploy| is used to specify the script to run when the testing is successful. A separate bash script \verb|deploy.sh| is defined for this purpose, inside which is a sequence of commands that builds and publishes images, and configures Kubernetes using \verb|kubectl|.

It is particularly worth mention that in \verb|deploy.sh|, when building and applying the latest version of the docker image, tagging using \verb|<image-name>:latest| alone is not going to work. This is because when the same configuration with \verb|<image-name>:latest| is applied, the system would simply acknowledge it as ``no change'' and would not actually download the latest version of the image.

The walk introduced earlier was to use version number in the configuration file and/or as an imperial command as follows.
\begin{lstlisting}
$ kubectl set image deployment/<Deployment name> <container name>=<image name>:<version>
\end{lstlisting}
so what when the version name changes, Kubernetes would notice the differences and apply the new image. When working with CI/CD using \textit{Git}, this can be further automated. Just use the \verb|$GIT_SHA| as part of the tag as follows.
\begin{lstlisting}
$ docker build -t <docker-id>/<image-name>:latest -t <docker-id>/<image-name>:$GIT_SHA -f <dockerfile> <save-directory>
$ docker push <docker-id>/<image-name>:latest
$ docker push <docker-id>/<image-name>:$GIT_SHA
\end{lstlisting}
Notice that in addition to \verb|:latest|, \verb|:$GIT_SHA| is used as a secondary tag. When pushing the built image to Docker Hub, both \verb|:latest| and \verb|:$GIT_SHA| are pushed (although they have identical content). When setting image, the \verb|$GIT_SHA| is used to identify the image just like the version number.

It is recommended not to remove \verb|:latest| in the building command. This is because if someone wants to pull and test the latest image in his server (without knowing the value of \verb|$GIT_SHA| for the latest commit), he is still able to do so using only the image name.

Notice that \verb|$GIT_SHA| is not a built-in environment variable. The developer needs to set that environmental variable manually in the configuration YAML file as follows. It is possible to replace \verb|$GIT_SHA| with a different name.
\begin{lstlisting}
env:
global:
- GIT_SHA=$(git rev-parse HEAD)
\end{lstlisting}
With the above setup, \verb|$GIT_SHA| can be used in \verb|deploy.sh| as an environmental variable.

\subsection{Manage Secrets}

Notice that when CI/CD tool is tied to the cloud platform provider, service account authentication is required. It is a good habit to NOT to put the authentication information in the CI/CD configuration file as plain text, or to upload the unencrypted file that contains the authentication information to the public workspace. It is possible that some CI/CD tools provide encryption tools that can be used to encrypt the authentication file. In such case, the developer may need to install the required CLI for that CI/CD tool.

In Section \ref{ch:vac:subsec:k8ssecrets}, it has been introduced that Kubernetes uses Secrets object to encrypt secret files. The encrypted secret files can then be safely published online. In the Kubernetes configuration file, an environmental variable can be created to call the secret information.

Many cloud platform providers including Google Cloud Platform provides services to manage secrets.

\subsection{Helm}

To install a software in a Kubernetes pod, the most intuitive way is to commit the installation in the image, and call the image in the Kubernetes configuration file. For commonly used services such as \verb|ingress-nginx|, its installation configuration file is available online as part of the manual. It essentially starts and initializes a branch of Kubernetes objects to enable the service.

Helm is designed as an alternative to manage software installation in Kubernetes clusters. In many occasions, it is more convenient (or even only possible) to use Helm to install a software. More details are given in \textit{github.com/helm/helm}.

We need to first install Helm from script. Helm installation used to contain two parts, the CLI (referred as Helm client) and the server (referred as Tiller server). We could then use Helm CLI to install other third-party software and tools.

Access control is important on cloud platforms. In practice, user accounts are used to identify users, and service accounts to identify pods and programs. Associated role bindings are used to manage what resources can be accessed by a user or program. For example, administrative role over the entire account can be used to bind with the administrative user. The same applies to Helm. The Tiller server required some extent of administrative control over the resources in an account. In many occasions, Tiller server was given the administrative permission to access the entire account, which introduced security risks.

As of Helm version 3, a major change was carried out where Tiller server was removed completely. Helm architecture is more secure and simpler today. The concerns related to Tiller's permission in the Kubernetes cluster are no longer relevant. Helm 3 of course requires permissions to use the resources, which is now managed by Kubernetes role-based access control mechanisms.

\section{Alternatives to Kubernetes}

Portainer is an open-source container orchestrator that has a user-friendly interface and is relatively easier to use than the more powerful and famous container orchestrator Kubernetes which is introduced in the next chapter. In this section, it is used as an example, just to give the reader an idea what container orchestrators in the market may look like. The section does not go into details about Portainer.

Before starting a Portainer container, it is a good practice to first create a docker volume for Portainer to store the database. Use the following command to create such docker volume.
\begin{lstlisting}
$ docker volume create portainer_data
\end{lstlisting}
Then run a Portainer container using
\begin{lstlisting}
$ docker run -d -p 8000:8000 -p 9000:9000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce
\end{lstlisting}
where ports 8000, 9000 and 9443 are used for hosting HTTP traffic in development environments, hosting web interface, and hosting HTTPS or SSL-secured services, respectively. The \verb|docker.sock| is the socket that enables the docker server-side daemon to communicate with its command-line interface. The image name for Portainer community edition (distinguished form the business edition) is \verb|portainer/portainer-ce|.

Use \verb|https://localhost:9443| to login to the container. The following page in Fig. \ref{ch:vac:fig:portainerlogin} should pop up in the first-time login, asking the user to create and administration user.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=350pt]{chapters/part-3/figures/portainerlogin.png}
	\caption{Portainer login page to create admin user.} \label{ch:vac:fig:portainerlogin}
\end{figure}

After creating the admin user and logging in, the status of images, containers and many more can be monitored via the dashboard, as shown in Figs. \ref{ch:vac:fig:portainerdashboard1}, \ref{ch:vac:fig:portainerdashboard2} and \ref{ch:vac:fig:portainerdashboard3}. Notice that in Fig. \ref{ch:vac:fig:portainerdashboard3}, using the ``quick action'' buttons, the user can check the specifics of the container and interact with its console, just like using \verb|docker container inspect| and \verb|docker exec|
\begin{figure}[htbp]
	\centering
	\includegraphics[width=350pt]{chapters/part-3/figures/portainerdashboard1.png}
	\caption{Portainer dashboard overview of docker servers.} \label{ch:vac:fig:portainerdashboard1}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=350pt]{chapters/part-3/figures/portainerdashboard2.png}
	\caption{Portainer dashboard overview in a docker server.} \label{ch:vac:fig:portainerdashboard2}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=350pt]{chapters/part-3/figures/portainerdashboard3.png}
	\caption{Portainer dashboard list down of all running containers.} \label{ch:vac:fig:portainerdashboard3}
\end{figure}

In summary, Portainer is an easy-to-use container management tool with clean graphical interface that a user can quickly get used to without a steep learning curve. 